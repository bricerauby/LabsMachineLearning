{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4-: Mixture Models+Model orden selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab session is to study mixture models. In the first part you will code the EM algorithm to estimate the parameters of a GMM given the number of mixed distributions and in the second part you will try different model order selection methods. You will send only one notebook for both parts **(this notebook will be updated with instructions for the second part and the deadline)**.\n",
    "\n",
    "You have to send the filled notebook named **\"L4__familyname1_familyname2.ipynb\"** (groups of 2) by email to aml.centralesupelec.2019@gmail.com and put **\"AML-L4- \"** in the subject. \n",
    "\n",
    "We begin with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_context('poster')\n",
    "sns.set_color_codes()\n",
    "plot_kwds = {'alpha' : 0.25, 's' : 80, 'linewidths':0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Gaussian mixture model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. After estimation of those parameters we get an estimation of the distribution of our data. For the clustering task, one can think of mixture models as generalizing k-means clustering to incorporate information about the covariance structure of the data as well as the centers of the latent Gaussians. \n",
    "\n",
    "### First part\n",
    "\n",
    "Fill in the following class to implement a multivariate GMM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(2, 3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 5, 5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(np.expand_dims(x, -1), np.expand_dims(x, axis = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(x, axis = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = np.random.rand(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.vectorize(multivariate_normal.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_normal.pdf(X, mean=self.mu_, cov=self.Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,] = np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class my_GMM():\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        '''\n",
    "        Parameters:\n",
    "        k: integer\n",
    "            number of components\n",
    "        \n",
    "        Attributes:\n",
    "        \n",
    "        alpha_: np.array\n",
    "            proportion of components\n",
    "        mu_: np.array\n",
    "            array containing means\n",
    "        Sigma_: np.array\n",
    "            array cointaining covariance matrix\n",
    "        cond_prob_: (n, K) np.array\n",
    "            conditional probabilities for all data points \n",
    "        labels_: (n, ) np.array\n",
    "            labels for data points\n",
    "        '''\n",
    "        self.k = k\n",
    "        self.alpha_ = None\n",
    "        self.mu_ = None\n",
    "        self.Sigma_ = None\n",
    "        self.cond_prob_ = None\n",
    "        self.labels_ = None\n",
    "        \n",
    "    def fit(self, X, initialization = 'random'):\n",
    "        \"\"\" Find the parameters\n",
    "        that better fit the data\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            Data matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        self\n",
    "        \"\"\"\n",
    "        n, p = X.shape\n",
    "        k = self.k\n",
    "        if self.initialization == 'random' :\n",
    "            self.mu_ = np.random.rand(k, p)\n",
    "            self.Sigma_ = np.zeros((k, p, p))\n",
    "            self.Sigma_[:,] = np.eye(p)\n",
    "            self.alpha_ = np.random.rand(k)\n",
    "            self.alpha_ = self.alpha_/np.sum(self.alpha_)\n",
    "        \n",
    "        def compute_condition_prob_matrix(X, alpha, mu, Sigma):\n",
    "            '''Compute the conditional probability matrix \n",
    "            shape: (n, K)\n",
    "            '''\n",
    "            self.cond_prob_ = np.zeros((n, k))\n",
    "            \n",
    "        # TODO:\n",
    "        # initialize the parameters\n",
    "        # apply sklearn kmeans or randomly initialize them\n",
    "        convergence = False\n",
    "        While not(convergence) :\n",
    "            density = np.zeros((n, k))\n",
    "            for i in range (k):\n",
    "                density[:, i] = multivariate_normal.pdf(X, mean=self.mu_[i], cov=self.Sigma[i])\n",
    "            self.cond_prob_ = self.alpha_.T * density\n",
    "            self.cond_prob_ = self.cond_prob_ / np.sum(self.cond_prob_, axis = 1)\n",
    "            \n",
    "            self.alpha_ = np.mean(self.cond_prob_, axis = 0)\n",
    "            self.mu_ = (1/self.alpha_) * np.mean( np.expand_dims(X, axis = 1) * self.cond_prob_, axis = 0)\n",
    "            tmp = np.expand_dims(X, axis=1) - np.expand_dims(self.mu_, axis = 0)\n",
    "            self.Sigma_ = (1/self.alpha_) * np.sum(p * np.matmul(np.expand_dims(tmp, -1), np.expand_dims(tmp, 2)),\n",
    "                                                   axis = 0)\n",
    "            ## RAJOUTER UN CRITERE DE CONVERGENCE \n",
    "            \n",
    "        #     Compute conditional probability matrix\n",
    "        #     Update parameters\n",
    "        \n",
    "        # Update labels_\n",
    "        \n",
    "        Return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict labels for X\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            New data matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        label assigment        \n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        \n",
    "    def compute_proba(self, X):\n",
    "        \"\"\" Compute probability vector for X\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: (n, p) np.array\n",
    "            New data matrix\n",
    "        \n",
    "        Returns:\n",
    "        -----\n",
    "        proba: (n, k) np.array        \n",
    "        \"\"\"\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate your own mixture of Gaussian distributions to test the model, choose parameters so that GMM performs better than K-Means on it. Use `np.random.multivariate_normal`. \n",
    "\n",
    "Plot data with colors representing predicted labels and shapes representing real labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus (not graded): Implement a mixture of asymmetric generalized Gaussians (AGGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Part\n",
    " \n",
    "To be updated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
